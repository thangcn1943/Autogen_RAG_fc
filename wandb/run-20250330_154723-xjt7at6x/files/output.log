WandB run initialized: Project - ft_for_rag, Run - llama3-fc-303
ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
/home/duyhoang/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.49.0.
   \\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.65 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.16it/s]
Unsloth 2025.3.19 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
Using a sample size of 15000 for fine-tuning.
GPU = NVIDIA GeForce RTX 4090. Max memory = 23.65 GB.
15.316 GB of memory reserved.
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 15,000 | Num Epochs = 1 | Total steps = 375
O^O/ \_/ \    Batch size per device = 20 | Gradient accumulation steps = 2
\        /    Data Parallel GPUs = 1 | Total batch size (20 x 2 x 1) = 40
 "-____-"     Trainable parameters = 83,886,080/8,114,147,328 (1.03% trained)
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
Unsloth: Will smartly offload gradients to save VRAM!
TrainOutput(global_step=375, training_loss=0.5729361518224081, metrics={'train_runtime': 6643.8022, 'train_samples_per_second': 2.258, 'train_steps_per_second': 0.056, 'total_flos': 6.568871551406899e+17, 'train_loss': 0.5729361518224081})
