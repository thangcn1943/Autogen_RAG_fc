{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/duyhoang/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthang19431\u001b[0m (\u001b[33mthang19431-hanoi-university-of-science-and-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged into WandB.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/data1tb/thangcn/datnv2/wandb/run-20250328_091616-gc0pbgf9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thang19431-hanoi-university-of-science-and-technology/ft_for_rag/runs/gc0pbgf9' target=\"_blank\">llama3-fc</a></strong> to <a href='https://wandb.ai/thang19431-hanoi-university-of-science-and-technology/ft_for_rag' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thang19431-hanoi-university-of-science-and-technology/ft_for_rag' target=\"_blank\">https://wandb.ai/thang19431-hanoi-university-of-science-and-technology/ft_for_rag</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thang19431-hanoi-university-of-science-and-technology/ft_for_rag/runs/gc0pbgf9' target=\"_blank\">https://wandb.ai/thang19431-hanoi-university-of-science-and-technology/ft_for_rag/runs/gc0pbgf9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB run initialized: Project - ft_for_rag, Run - llama3-fc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/mnt/data1tb/thangcn/datnv2/.env')\n",
    "\n",
    "def setup_wandb(project_name: str, run_name: str):\n",
    "    # Set up your API KEY\n",
    "    try:\n",
    "        api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "        wandb.login(key=api_key)\n",
    "        print(\"Successfully logged into WandB.\")\n",
    "    except KeyError:\n",
    "        raise EnvironmentError(\"WANDB_API_KEY is not set in the environment variables.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error logging into WandB: {e}\")\n",
    "    \n",
    "    # Optional: Log models\n",
    "    os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
    "    os.environ[\"WANDB_WATCH\"] = \"all\"\n",
    "    os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "    \n",
    "    # Initialize the WandB run\n",
    "    try:\n",
    "        wandb.init(project=project_name, name=run_name)\n",
    "        print(f\"WandB run initialized: Project - {project_name}, Run - {run_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing WandB run: {e}\")\n",
    "\n",
    "\n",
    "setup_wandb(project_name=\"ft_for_rag\", run_name=\"llama3-fc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duyhoang/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"thang1943/Llama-3-8B-Instruct-Finance-RAG\"\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, quantization_config=quantization_config, device_map=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128264, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128264, bias=False)\n",
      ")\n",
      "LlamaModel(\n",
      "  (embed_tokens): Embedding(128264, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "        (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "        (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "        (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "        (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "        (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "Embedding(128264, 4096)\n",
      "ModuleList(\n",
      "  (0-31): 32 x LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      ")\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "LlamaAttention(\n",
      "  (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "LlamaMLP(\n",
      "  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "SiLU()\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRMSNorm((4096,), eps=1e-05)\n",
      "LlamaRotaryEmbedding()\n",
      "Linear(in_features=4096, out_features=128264, bias=False)\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=32,   # LoRA rank - suggested values: 8, 16, 32, 64, 128\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  \n",
    "    bias=\"none\",    \n",
    "    use_rslora=False,   \n",
    "    loftq_config=None  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 83,886,080 || all params: 8,114,212,864 || trainable%: 1.0338\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(model, config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = peft_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "peft_model = prepare_model_for_kbit_training(\n",
    "    peft_model,\n",
    "    use_gradient_checkpointing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a sample size of 15000 for fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Loading the dataset\n",
    "dataset = load_dataset(\"Salesforce/xlam-function-calling-60k\", split=\"train\")\n",
    "dataset = dataset.select(range(15000))\n",
    "print(f\"Using a sample size of {len(dataset)} for fine-tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'query', 'answers', 'tools'],\n",
      "    num_rows: 15000\n",
      "})\n",
      "{'id': 0, 'query': 'Where can I find live giveaways for beta access and games?', 'answers': '[{\"name\": \"live_giveaways_by_type\", \"arguments\": {\"type\": \"beta\"}}, {\"name\": \"live_giveaways_by_type\", \"arguments\": {\"type\": \"game\"}}]', 'tools': '[{\"name\": \"live_giveaways_by_type\", \"description\": \"Retrieve live giveaways from the GamerPower API based on the specified type.\", \"parameters\": {\"type\": {\"description\": \"The type of giveaways to retrieve (e.g., game, loot, beta).\", \"type\": \"str\", \"default\": \"game\"}}}]'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>query</th>\n",
       "      <th>answers</th>\n",
       "      <th>tools</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Where can I find live giveaways for beta acces...</td>\n",
       "      <td>[{\"name\": \"live_giveaways_by_type\", \"arguments...</td>\n",
       "      <td>[{\"name\": \"live_giveaways_by_type\", \"descripti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I need to understand the details of the Ethere...</td>\n",
       "      <td>[{\"name\": \"web_chain_details\", \"arguments\": {\"...</td>\n",
       "      <td>[{\"name\": \"peers\", \"description\": \"Retrieves a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What is the T3MA for 'ETH/BTC' using a 1h inte...</td>\n",
       "      <td>[{\"name\": \"t3ma\", \"arguments\": {\"symbol\": \"ETH...</td>\n",
       "      <td>[{\"name\": \"t3ma\", \"description\": \"Fetches the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>List titles originally aired on networks '1' a...</td>\n",
       "      <td>[{\"name\": \"list_titles\", \"arguments\": {\"networ...</td>\n",
       "      <td>[{\"name\": \"get_animes\", \"description\": \"Retrie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Fetch the competitor standings for the recentl...</td>\n",
       "      <td>[{\"name\": \"stagecompetitorstandings\", \"argumen...</td>\n",
       "      <td>[{\"name\": \"stagecompetitorstandings\", \"descrip...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              query  \\\n",
       "0   0  Where can I find live giveaways for beta acces...   \n",
       "1   1  I need to understand the details of the Ethere...   \n",
       "2   2  What is the T3MA for 'ETH/BTC' using a 1h inte...   \n",
       "3   3  List titles originally aired on networks '1' a...   \n",
       "4   4  Fetch the competitor standings for the recentl...   \n",
       "\n",
       "                                             answers  \\\n",
       "0  [{\"name\": \"live_giveaways_by_type\", \"arguments...   \n",
       "1  [{\"name\": \"web_chain_details\", \"arguments\": {\"...   \n",
       "2  [{\"name\": \"t3ma\", \"arguments\": {\"symbol\": \"ETH...   \n",
       "3  [{\"name\": \"list_titles\", \"arguments\": {\"networ...   \n",
       "4  [{\"name\": \"stagecompetitorstandings\", \"argumen...   \n",
       "\n",
       "                                               tools  \n",
       "0  [{\"name\": \"live_giveaways_by_type\", \"descripti...  \n",
       "1  [{\"name\": \"peers\", \"description\": \"Retrieves a...  \n",
       "2  [{\"name\": \"t3ma\", \"description\": \"Fetches the ...  \n",
       "3  [{\"name\": \"get_animes\", \"description\": \"Retrie...  \n",
       "4  [{\"name\": \"stagecompetitorstandings\", \"descrip...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant with tools. Use these when needed:\n",
      "[{\"name\": \"live_giveaways_by_type\", \"description\": \"Retrieve live giveaways from the GamerPower API based on the specified type.\", \"parameters\": {\"type\": {\"description\": \"The type of giveaways to retrieve (e.g., game, loot, beta).\", \"type\": \"str\", \"default\": \"game\"}}}]<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Where can I find live giveaways for beta access and games?<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "[{\"name\": \"live_giveaways_by_type\", \"arguments\": {\"type\": \"beta\"}}, {\"name\": \"live_giveaways_by_type\", \"arguments\": {\"type\": \"game\"}}]<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set chat template cho Llama-3 (nếu chưa có)\n",
    "llama3_template = \"\"\"{% for message in messages %}\n",
    "{% if message['role'] == 'system' %}\n",
    "{{ '<|start_header_id|>system<|end_header_id|>\\n\\n' + message['content'] + '<|eot_id|>' }}\n",
    "{% elif message['role'] == 'user' %}\n",
    "{{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + message['content'] + '<|eot_id|>' }}\n",
    "{% elif message['role'] == 'assistant' %}\n",
    "{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' + message['content'] + '<|eot_id|>' }}\n",
    "{% endif %}\n",
    "{% endfor %}\n",
    "{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\"\"\"\n",
    "\n",
    "tokenizer.chat_template = llama3_template\n",
    "\n",
    "# Hàm format dữ liệu\n",
    "def format_conversation(examples):\n",
    "    formatted_texts = []\n",
    "    for query, tools, answer in zip(examples['query'], examples['tools'], examples['answers']):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": f\"You are a helpful assistant with tools. Use these when needed:\\n{tools}\"},\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "            {\"role\": \"assistant\", \"content\": answer}\n",
    "        ]\n",
    "        formatted = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        formatted_texts.append(formatted)\n",
    "    return {\"text\": formatted_texts}\n",
    "\n",
    "# Load và xử lý dataset\n",
    "dataset = load_dataset(\"Salesforce/xlam-function-calling-60k\", split=\"train\").select(range(15000))\n",
    "dataset = dataset.map(format_conversation, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "# Kiểm tra kết quả\n",
    "print(dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "        output_dir = \"outputs\",             \n",
    "        per_device_train_batch_size = 8,  # Controls the batch size per device\n",
    "        gradient_accumulation_steps = 2,  # Accumulates gradients to simulate a larger batch\n",
    "        warmup_steps = 5,\n",
    "        learning_rate = 2e-4,             # Sets the learning rate for optimization\n",
    "        num_train_epochs = 3,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,              # Regularization term for preventing overfitting\n",
    "        lr_scheduler_type = \"linear\",     # Chooses a linear learning rate decay\n",
    "        seed = 3407,                        \n",
    "        report_to = \"wandb\",              # Enables Weights & Biases (W&B) logging\n",
    "        logging_steps = 1,                # Sets frequency of logging to W&B\n",
    "        logging_strategy = \"steps\",       # Logs metrics at each specified step\n",
    "        save_strategy = \"no\",               \n",
    "        load_best_model_at_end = True,    # Loads the best model at the end\n",
    "        save_only_model = False           # Saves entire model, not only weights\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = peft_model,\n",
    "    processing_class = tokenizer,\n",
    "    train_dataset = dataset,      \n",
    "    args = args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4090. Max memory = 23.65 GB.\n",
      "9.904 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "device() received an invalid combination of arguments - got (NoneType), but expected one of:\n * (torch.device device)\n      didn't match because some of the arguments have invalid types: (!NoneType!)\n * (str type, int index = -1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thangcn/lib/python3.10/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thangcn/lib/python3.10/site-packages/transformers/trainer.py:2365\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2363\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mprepare(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m   2364\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2365\u001b[0m         model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2366\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2367\u001b[0m     \u001b[38;5;66;03m# to handle cases wherein we pass \"DummyScheduler\" such as when it is specified in DeepSpeed config.\u001b[39;00m\n\u001b[1;32m   2368\u001b[0m     model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mprepare(\n\u001b[1;32m   2369\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\n\u001b[1;32m   2370\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/thangcn/lib/python3.10/site-packages/accelerate/accelerator.py:1398\u001b[0m, in \u001b[0;36mAccelerator.prepare\u001b[0;34m(self, device_placement, *args)\u001b[0m\n\u001b[1;32m   1396\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp8_backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSAMP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1397\u001b[0m         args, device_placement \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_msamp(\u001b[38;5;241m*\u001b[39margs, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n\u001b[0;32m-> 1398\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1399\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_pass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1400\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1401\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_one(obj, device_placement\u001b[38;5;241m=\u001b[39md) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, device_placement))\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tpu_should_fix_optimizer:\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;66;03m# 2. grabbing new model parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/thangcn/lib/python3.10/site-packages/accelerate/accelerator.py:1399\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1396\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp8_backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSAMP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1397\u001b[0m         args, device_placement \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_msamp(\u001b[38;5;241m*\u001b[39margs, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n\u001b[1;32m   1398\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m-> 1399\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_pass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, device_placement)\n\u001b[1;32m   1400\u001b[0m     )\n\u001b[1;32m   1401\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_one(obj, device_placement\u001b[38;5;241m=\u001b[39md) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, device_placement))\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tpu_should_fix_optimizer:\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;66;03m# 2. grabbing new model parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/thangcn/lib/python3.10/site-packages/accelerate/accelerator.py:1272\u001b[0m, in \u001b[0;36mAccelerator._prepare_one\u001b[0;34m(self, obj, first_pass, device_placement)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_data_loader(obj, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m-> 1272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer):\n\u001b[1;32m   1274\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_optimizer(obj, device_placement\u001b[38;5;241m=\u001b[39mdevice_placement)\n",
      "File \u001b[0;32m~/miniconda3/envs/thangcn/lib/python3.10/site-packages/accelerate/accelerator.py:1501\u001b[0m, in \u001b[0;36mAccelerator.prepare_model\u001b[0;34m(self, model, device_placement, evaluation_mode)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_bitsandbytes_multi_backend_available():\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;66;03m# bnb with multi-backend supports CPU which don't need to check index.\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m-> 1501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_device_index\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice:\n\u001b[1;32m   1502\u001b[0m     \u001b[38;5;66;03m# if on the first device (GPU 0) we don't care\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (current_device_index \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m   1504\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1505\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt train a model that has been loaded in 8-bit or 4-bit precision on a different device than the one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1506\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre training on. Make sure you loaded the model on the correct device using for example `device_map=\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:torch.cuda.current_device()}` or `device_map=\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:torch.xpu.current_device()}`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1507\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: device() received an invalid combination of arguments - got (NoneType), but expected one of:\n * (torch.device device)\n      didn't match because some of the arguments have invalid types: (!NoneType!)\n * (str type, int index = -1)\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thangcn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
